{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DataStax Academy](https://s3.amazonaws.com/datastaxtraining/vq8Jr36Gk48v/datastax-academy.svg \"DataStax Academy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 06.02 - Tuning Partitioning: Partitioning Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll be exploring partitioning properties for RDDs created from different sources and transformations.\n",
    "\n",
    "You'll be working with the `videos` table:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE TABLE killr_video.videos (\n",
    "    video_id TIMEUUID,\n",
    "    avg_rating FLOAT,\n",
    "    description TEXT,\n",
    "    genres SET<TEXT>,\n",
    "    mpaa_rating TEXT,\n",
    "    release_date TIMESTAMP,\n",
    "    release_year INT,\n",
    "    title TEXT,\n",
    "    user_id UUID,\n",
    "    PRIMARY KEY (video_id)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a text file, `/root/data/video-ids.csv`, which contains only one column: the video IDs for different movies. Some of the video IDs are orphaned, i.e. not in the `killrvideo.videos` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create an RDD from the text file. Hint: map() each row to a UUID via java.util.UUID.fromString(). What do you expect the partitioner and number of partitions to be? Why? Write Scala code to discover the actual values. Were they what you expected them to be? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "val videoIds = sc.textFile(\"file:///root/data/video-ids.csv\").map(id => java.util.UUID.fromString(id))\n",
    "\n",
    "println(videoIds.partitioner)\n",
    "println(videoIds.partitions.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioner is `None` because our data source is a text file. The partition size is 2 because that's the number of cores thus that's the value of `defaultParallelism`. `defaultParallelism` is greater in this case because we have less than 64 MB of data in the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create an RDD from the killrvideo.videos table select()ing only the videoids. What do you expect the partitioner and number of partitions to be? Why? Write Scala code to discover the actual values. Were they what you expected them to be? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "val killrVideoIds = sc.cassandraTable[java.util.UUID](\"killr_video\", \"videos\").select(\"video_id\")\n",
    "\n",
    "println(killrVideoIds.partitioner)\n",
    "println(killrVideoIds.partitions.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitioner is `None` because it is a C\\* source.\n",
    "`partitions.size` is 1. This reflects the number of nodes in the C* cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Find the orphaned video IDs by using subtract(). What do you expect the partitioner and number of partitions to be? Why? Write Scala code to discover the actual values. Were they what you expected them to be? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "val orphans = videoIds.subtract(killrVideoIds)\n",
    "\n",
    "println(orphans.partitioner)\n",
    "println(orphans.partitions.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitioner is still `None` because we use the source's partitioner. Both inputs here have a partitioner of `None`.\n",
    "The number of partitions is 2 because the number of partitions for the text file RDD is 2 (we take the source RDD's number of partitions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Turn your RDD into a RDD of keys via keyBy() then sort using sortByKey(). What do you expect the partitioner and number of partitions to be? Why? Write Scala code to discover the actual values. Were they what you expected them to be? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some(org.apache.spark.RangePartitioner@daac8b75)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "val sortedOrphans = orphans.keyBy(row => row).sortByKey()\n",
    "\n",
    "println(sortedOrphans.partitioner)\n",
    "println(sortedOrphans.partitions.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a RangePartitioner because we used `sortByKey`. The number of partitions is still 2 because it's the parent's number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Display the resulting orphaned video IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9056808b-ca65-1bfb-9957-3bea148dfdce,9056808b-ca65-1bfb-9957-3bea148dfdce)\n",
      "(907df86e-2208-18a8-90aa-6d837c659f2f,907df86e-2208-18a8-90aa-6d837c659f2f)\n",
      "(9646278f-14bd-11e5-88ea-8438355b7e3a,9646278f-14bd-11e5-88ea-8438355b7e3a)\n",
      "(9db57288-e51c-1ff1-805d-c5f1e49c2c8b,9db57288-e51c-1ff1-805d-c5f1e49c2c8b)\n",
      "(fe3c4045-6f37-1223-81be-250dc60cffc8,fe3c4045-6f37-1223-81be-250dc60cffc8)\n",
      "(2645e79c-14bd-11e5-a456-8438355b7e3a,2645e79c-14bd-11e5-a456-8438355b7e3a)\n",
      "(264601a3-14bd-11e5-8c2e-8438355b7e3a,264601a3-14bd-11e5-8c2e-8438355b7e3a)\n",
      "(2646123a-14bd-11e5-b9db-8438355b7e3a,2646123a-14bd-11e5-b9db-8438355b7e3a)\n",
      "(26461a70-14bd-11e5-ad08-8438355b7e3a,26461a70-14bd-11e5-ad08-8438355b7e3a)\n",
      "(2e8ecb4f-e92b-139b-8183-4df0e2a817bb,2e8ecb4f-e92b-139b-8183-4df0e2a817bb)\n"
     ]
    }
   ],
   "source": [
    "sortedOrphans.collect.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
